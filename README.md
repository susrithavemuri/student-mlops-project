# ğŸš€ Student Performance MLOps Pipeline
 Production-Grade End-to-End Machine Learning System
# ğŸ“Œ Overview

This project demonstrates a production-ready MLOps pipeline for student performance prediction, covering the complete ML lifecycle:

Model training and evaluation

Experiment tracking with MLflow (SQLite backend)

Model registry & version control

Production model artifact export

Drift detection & monitoring (Evidently AI)

Automatic retraining trigger

Dockerized inference API (FastAPI)

CI/CD automation via GitHub Actions

The system is designed to reflect real-world ML deployment practices used in industry and research environments.

# ğŸ— Architecture
Training Pipeline
        â†“
MLflow Tracking (SQLite DB)
        â†“
Model Registry & Production Approval
        â†“
Production Model Export
        â†“
Dockerized FastAPI Inference Service
        â†“
Monitoring & Drift Detection
        â†“
Auto-Retraining Trigger


This architecture separates:

Training environment

Model management

Inference deployment

Monitoring layer

â€” following best practices in modern MLOps.

# ğŸ§  Problem Statement

Predict student performance outcomes based on structured academic features such as:

Study hours

Attendance

Academic metrics

The objective is not only to build a predictive model but to ensure:

Reproducibility

Observability

Model lifecycle management

Deployment robustness

# âš™ï¸ Tech Stack

| Layer               | Technology              |
| ------------------- | ----------------------- |
| ML Model            | Scikit-learn            |
| Experiment Tracking | MLflow (SQLite backend) |
| Model Registry      | MLflow Model Registry   |
| Monitoring          | Evidently AI            |
| API                 | FastAPI                 |
| Containerization    | Docker                  |
| CI/CD               | GitHub Actions          |
| Language            | Python 3.11             |


# ğŸ“Š Key MLOps Features
## âœ… Experiment Tracking

All training runs logged to MLflow

Parameters, metrics, artifacts stored in SQLite DB

## âœ… Model Registry

Versioned model management

Production alias control

## âœ… Production Artifact Export

Clean separation of training and inference

No registry dependency in inference container

## âœ… Data Drift Detection

Statistical comparison between reference and incoming data

Drift detection report generation (HTML)

## âœ… Auto-Retraining Trigger

Drift detection can automatically trigger retraining pipeline

## âœ… Dockerized Deployment

Containerized inference API

Portable and reproducible environment

## âœ… CI/CD Automation

GitHub Actions pipeline:

Install dependencies

Run training

Execute monitoring checks

# â–¶ï¸ How to Run Locally

1ï¸âƒ£ Train Model
python src/train.py

2ï¸âƒ£ Launch MLflow UI
python -m mlflow ui --backend-store-uri sqlite:///mlflow.db


Open:

http://127.0.0.1:5000

3ï¸âƒ£ Run API
uvicorn src.api:app --reload


Open:

http://127.0.0.1:8000/docs

4ï¸âƒ£ Run Drift Monitoring
python src/monitor.py


Generates:

drift_report.html

ğŸ³ Run with Docker

Build image:

docker build -t student-ml-api .


Run container:

docker run -p 8000:8000 student-ml-api


Open:

http://localhost:8000

ğŸ” CI/CD Workflow

On every push to main:

Install dependencies

Train model

Run monitoring script

Ensures pipeline integrity and reproducibility.

# ğŸ“ Research & Academic Relevance

This project demonstrates concepts central to modern ML research and production systems:

Reproducibility through environment isolation

Database-backed experiment tracking

Model lifecycle management

Data drift detection in non-stationary environments

Automated retraining workflows

Deployment architecture separation

It reflects applied understanding of:

Reliable ML systems

Observability in production models

End-to-end ML system design

# ğŸ“ˆ Future Improvements

Performance degradation monitoring

Concept drift detection

Cloud deployment (AWS / GCP)

PostgreSQL-backed MLflow

Feature store integration

Canary deployment strategy
